version: "3.1"

services:
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka0
    ports:
      - 8080:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka0:29092
      DYNAMIC_CONFIG_ENABLED: "true"
  kafka0:
    container_name: kafka
    image: confluentinc/cp-kafka:7.2.1.arm64
    ports:
      - 9092:9092
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "9092" ]
      interval: 10s
      timeout: 5s
      retries: 3
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka0:29092,PLAINTEXT_HOST://kafka0:9092
      KAFKA_LISTENERS: PLAINTEXT://kafka0:29092,CONTROLLER://kafka0:29093,PLAINTEXT_HOST://:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka0:29093"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
    volumes:
      - ./build/dev/kafka/scripts/run_workaround.sh:/tmp/run_workaround.sh
    command: "bash -c '/tmp/run_workaround.sh && /etc/confluent/docker/run'"
  kafka-init-topics:
    container_name: init-topics
    image: confluentinc/cp-kafka:7.2.1.arm64
    depends_on:
      - kafka0
    command: "bash -c 'echo Waiting for Kafka to be ready... && \
      cub kafka-ready -b kafka0:29092 1 30 && \
      kafka-topics --create --topic accounting-events --partitions 2 --replication-factor 1 --if-not-exists --bootstrap-server kafka0:29092'"
  rsend:
    image: rsend
    build: ./rsend/
    ports:
      - "8081:8081"
    depends_on:
      kafka0:
        condition: service_healthy
  transfer:
    image: transfer
    build: ./transfer/
    ports:
      - "8082:8082"
    depends_on:
      kafka0:
        condition: service_healthy

